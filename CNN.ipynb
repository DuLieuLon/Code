{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"wgGywTOdWdAr"},"source":["# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt ƒë·ªÉ ch·∫°y ch∆∞∆°ng tr√¨nh\n","%tensorflow_version 1.x\n","!pip install pyvi\n","!pip install ftfy\n","!pip install gensim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jqTDegKl3aDo"},"source":["# ƒê∆∞·ªùng d·∫´n ƒë∆∞·ª£c l∆∞u tr√™n google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","path_train ='/content/gdrive/My Drive/ABSA/Dataset/Train.txt'\n","path_dev ='/content/gdrive/My Drive/ABSA/Dataset/Dev.txt'\n","path_test ='/content/gdrive/My Drive/ABSA/Dataset/Test.txt'\n","\n","\n","# C√°c h√†m embedding kh√°c nhau theo th·ª≠ nghi·ªám c·ªßa b√†i b√°o\n","embedding_path = '/content/gdrive/My Drive/ABSA/Embedding/W2V_ner.vec'\n","#embedding_path = '/content/gdrive/My Drive/ABSA/Embedding/Bert_Base_ner.vec'\n","#embedding_path = '/content/gdrive/My Drive/ABSA/Embedding/ELMO_ner.vec'\n","#embedding_path = '/content/gdrive/My Drive/ABSA/Embedding/MULTI_W_F_B_E.vec'\n","#embedding_path = '/content/gdrive/My Drive/ABSA/Embedding/W2V_C2V_ner.vec'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AN8rgbmrdKJP"},"source":["# -*- coding: utf-8 -*-\n","import numpy as np\n","import tensorflow as tf\n","import os, pickle, re, keras, sklearn, string\n","from keras.callbacks import *\n","from keras.preprocessing.text import *\n","from keras.preprocessing.sequence import *\n","from keras.models import load_model\n","from pyvi import ViTokenizer, ViPosTagger\n","from keras.models import Model\n","from keras.layers import *\n","from keras import optimizers\n","from keras.utils.vis_utils import plot_model\n","import operator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Skg7lUZ9LYAn"},"source":["# H√†m ƒë·ªçc pretrained embedding \n","import io\n","def load_vectors(fname):\n","    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n","    count_vocab = 0\n","    n, d = map(int, fin.readline().split())\n","    data = {}\n","    for line in fin:\n","        tokens = line.rstrip().split(' ')\n","        coefs = np.asarray(tokens[1:], dtype='float64')\n","        data[tokens[0]] = coefs\n","        count_vocab += 1\n","    print(\"T·ªïng s·ªë c√°c t·ª´ v·ª±ng l√†: \" + str(count_vocab))\n","    return data\n","\n","word_embedding = load_vectors(embedding_path)\n","EMBEDDING_DIM = word_embedding['y√™u'].shape[0]\n","print(\"Chi·ªÅu c·ªßa embedding : \", EMBEDDING_DIM)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LZCbMBgRWdAy"},"source":["class Review():\n","    def __init__(self,stt,text,label):\n","        self.stt = stt\n","        self.labels = label\n","        self.orginalText = text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L1_TFsm9WdA0"},"source":["# C√°c h√†m ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n","def normalText(sent):\n","    sent = str(sent).replace('_',' ').replace('/',' tr√™n ')\n","    sent = re.sub('-{2,}','',sent)\n","    sent = re.sub('\\\\s+',' ', sent)\n","    patPrice = r'([0-9]+k?(\\s?-\\s?)[0-9]+\\s?(k|K))|([0-9]+(.|,)?[0-9]+\\s?(tri·ªáu|ng√†n|trƒÉm|k|K|))|([0-9]+(.[0-9]+)?√Ñ‚Äò)|([0-9]+k)'\n","    patURL = r\"(?:http://|www.)[^\\\"]+\"\n","    sent = re.sub(patURL,'website',sent)\n","    sent = re.sub(patPrice, ' gi√°_ti·ªÅn ', sent)\n","    sent = re.sub('\\.+','.',sent)\n","    sent = re.sub('(hagtag\\\\s+)+',' hagtag ',sent)\n","    sent = re.sub('\\\\s+',' ',sent)\n","    return sent\n","\n","def deleteIcon(text):\n","    text = text.lower()\n","    s = ''\n","    pattern = r\"[a-zA-ZaƒÉ√¢bcdƒëe√™ghiklmno√¥∆°pqrstu∆∞vxy√†·∫±·∫ßbcdƒë√®·ªÅgh√¨klmn√≤·ªì·ªùpqrst√π·ª´vx·ª≥√°·∫Ø·∫•bcdƒë√©·∫øgh√≠klmn√≥·ªë·ªõpqrst√∫·ª©vx√Ω·∫£·∫≥·∫©bcdƒë·∫ª·ªÉgh·ªâklmn·ªè·ªï·ªüpqrst·ªß·ª≠vx·ª∑·∫°·∫∑·∫≠bcdƒë·∫π·ªágh·ªãklmn·ªç·ªô·ª£pqrst·ª•·ª±vx·ªµ√£·∫µ·∫´bcdƒë·∫Ω·ªÖghƒ©klmn√µ·ªó·ª°pqrst≈©·ªØvx·ªπAƒÇ√ÇBCDƒêE√äGHIKLMNO√î∆†PQRSTU∆ØVXY√Ä·∫∞·∫¶BCDƒê√à·ªÄGH√åKLMN√í·ªí·ªúPQRST√ô·ª™VX·ª≤√Å·∫Æ·∫§BCDƒê√â·∫æGH√çKLMN√ì·ªê·ªöPQRST√ö·ª®VX√ù·∫†·∫∂·∫¨BCDƒê·∫∏·ªÜGH·ªäKLMN·ªå·ªò·ª¢PQRST·ª§·ª∞VX·ª¥·∫¢·∫≤·∫®BCDƒê·∫∫·ªÇGH·ªàKLMN·ªé·ªî·ªûPQRST·ª¶·ª¨VX·ª∂√É·∫¥·∫™BCDƒê·∫º·ªÑGHƒ®KLMN√ï·ªñ·ª†PQRST≈®·ªÆVX·ª∏,._]\"\n","    \n","    for char in text:\n","        if char !=' ':\n","            if len(re.findall(pattern, char)) != 0:\n","                s+=char\n","            elif char == '_':\n","                s+=char\n","        else:\n","            s+=char\n","    s = re.sub('\\\\s+',' ',s)\n","    return s.strip()\n","\n","\n","def clean_doc(doc):\n","    for punc in string.punctuation:\n","        doc = doc.replace(punc,' '+ punc + ' ')\n","    doc = normalText(doc)\n","    doc = deleteIcon(doc)\n","    doc = ViTokenizer.tokenize(doc)\n","    # Lowercase\n","    doc = doc.lower()\n","    # Removing multiple whitespaces\n","    doc = re.sub(r\"\\?\", \" \\? \", doc)\n","    # Remove numbers\n","    doc = re.sub(r\"[0-9]+\", \" num \", doc)\n","    # Split in tokens\n","    # Remove punctuation\n","    for punc in string.punctuation:\n","      if punc not in \"_\":\n","          doc = doc.replace(punc,' ')\n","    doc = re.sub('\\\\s+',' ',doc)\n","    return doc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Usy-4ygPWdA2"},"source":["# h√†m ƒë·ªçc d·ªØ li·ªáu l√™n t·ª´ file .txt theo ƒë·ªãnh d·∫°ng c·ªßa d·ªØ li·ªáu\n","from ftfy import fix_text\n","def read_files(filename):\n","    documents = list()\n","    count = 0\n","    with open(filename,'r',encoding='utf8') as file:\n","        text1 = file.read()\n","        for item in text1.split('\\n'):\n","            if item != '':\n","                item = item.strip()\n","                if count == 0:\n","                    stt = fix_text(item)\n","                    count += 1\n","                elif count == 1:\n","                    text = fix_text(item)\n","                    count += 1\n","                elif count == 2:\n","                    labels = fix_text(item)\n","                    count = 0\n","                    reviewtemp = Review(stt, text, labels)\n","                    documents.append(reviewtemp)\n","    return documents"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fyIQeZi-WdA4"},"source":["# H√†m chuy·ªÉn c√°c nh√£n th√†nh c√°c vector ƒë∆∞a v√†o m√¥ h√¨nh m√°y h·ªçc\n","def to_category_vector(label):\n","    vector = np.zeros(4).astype(np.float64)\n","    if label.strip() != '':\n","        if 'positive' in label:\n","            vector[1] = 1.0\n","        elif 'neutral' in label:\n","            vector[2] = 1.0\n","        elif 'negative' in label:\n","            vector[3] = 1.0\n","        else:\n","            vector[0] = 1.0\n","    else:\n","        vector[0] = 1.0\n","    return vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XvCVg0BsWdA6"},"source":["# ƒêo·∫°n code n√†y s·∫Ω chuy·ªÉn t·∫•t c·∫£ nh√£n th√†nh m·ªôt danh s√°ch c√°c vector bi·ªÉu di·ªÖn t∆∞∆°ng ·ª©ng\n","import re\n","def create_output(labels,categories,list_output):\n","    for i,category in enumerate(categories):\n","        if category in labels:\n","            try:\n","                output = re.findall('('+category+',\\s(positive|negative|neutral))',labels)[0][0]\n","            except:\n","                output = ''\n","            list_output[i].append(to_category_vector(output))\n","        else:\n","            list_output[i].append(to_category_vector(''))\n","    return list_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9XN2TgIzWdA8"},"source":["# ƒêo·∫°n code ƒë·ªçc d·ªØ li·ªáu t·ª´ c√°c file .txt c·ªßa t·∫≠p d·ªØ li·ªáu train, dev, test. \n","# Sau ƒë√≥ chuy·ªÉn nh√£n d∆∞·ªõi d·∫°ng text th√†nh d·∫°ng vector l∆∞u v√†o bi·∫øn.\n","\n","listLabel = 'FOOD#STYLE&OPTIONS,FOOD#PRICES,FOOD#QUALITY,DRINKS#STYLE&OPTIONS,DRINKS#QUALITY,DRINKS#PRICES,RESTAURANT#GENERAL,RESTAURANT#MISCELLANEOUS,RESTAURANT#PRICES,LOCATION#GENERAL,SERVICE#GENERAL,AMBIENCE#GENERAL'\n","categories = listLabel.split(',')\n","print(len(categories))\n","document_X_train = []\n","document_Y_train = []\n","document_X_dev = []\n","document_Y_dev = []\n","document_X_test = []\n","document_Y_test = []\n","\n","output_train1 = []\n","output_train1 = []\n","output_train2 = []\n","output_train3 = []\n","output_train4 = []\n","output_train5 = []\n","output_train6 = []\n","output_train7 = []\n","output_train8 = []\n","output_train9 = []\n","output_train10 = []\n","output_train11 = []\n","output_train12 = []\n","\n","list_output_train = list()\n","list_output_train.append(output_train1)\n","list_output_train.append(output_train2)\n","list_output_train.append(output_train3)\n","list_output_train.append(output_train4)\n","list_output_train.append(output_train5)\n","list_output_train.append(output_train6)\n","list_output_train.append(output_train7)\n","list_output_train.append(output_train8)\n","list_output_train.append(output_train9)\n","list_output_train.append(output_train10)\n","list_output_train.append(output_train11)\n","list_output_train.append(output_train12)\n","\n","for review in read_files(path_train):\n","    try:\n","        document_X_train.append(review.orginalText)\n","        list_output_train = create_output(review.labels, categories,list_output_train)\n","    except:\n","        print(review.labels)\n","        print(review.orginalText)\n","\n","output_dev1 = []\n","output_dev2 = []\n","output_dev3 = []\n","output_dev4 = []\n","output_dev5 = []\n","output_dev6 = []\n","output_dev7 = []\n","output_dev8 = []\n","output_dev9 = []\n","output_dev10 = []\n","output_dev11 = []\n","output_dev12 = []\n","list_output_dev = list()\n","list_output_dev.append(output_dev1)\n","list_output_dev.append(output_dev2)\n","list_output_dev.append(output_dev3)\n","list_output_dev.append(output_dev4)\n","list_output_dev.append(output_dev5)\n","list_output_dev.append(output_dev6)\n","list_output_dev.append(output_dev7)\n","list_output_dev.append(output_dev8)\n","list_output_dev.append(output_dev9)\n","list_output_dev.append(output_dev10)\n","list_output_dev.append(output_dev11)\n","list_output_dev.append(output_dev12)\n","for review in read_files(path_dev):\n","    document_X_dev.append(review.orginalText)\n","    list_output_dev = create_output(review.labels, categories,list_output_dev)\n","\n","for review in read_files(path_test):\n","    document_X_test.append(review.orginalText)    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fqYyTPmLWdA_"},"source":["# Ch·∫°y h√†m ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu (clean_doc) tr√™n to√†n b·ªô t·∫≠p d·ªØ li·ªáu train v√† t·∫≠p d·ªØ li·ªáu test.\n","totalX_train = []\n","totalY_train = np.array(document_Y_train)\n","\n","totalX_test = []\n","totalY_test = np.array(document_Y_test)\n","\n","totalX_dev = []\n","totalY_dev = np.array(document_Y_dev)\n","\n","for i, doc in enumerate(document_X_train):\n","    totalX_train.append(clean_doc(doc))\n","for i, doc in enumerate(document_X_test):\n","    totalX_test.append(clean_doc(doc))\n","\n","for i, doc in enumerate(document_X_dev):\n","    totalX_dev.append(clean_doc(doc))\n","print(totalX_train[0])\n","print(totalX_train[1])\n","print(totalX_train[2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0EB_NYPlWdBC"},"source":["# ƒê√¢y l√† h√†m chuy·ªÉn c√°c d·ªØ li·ªáu vƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c ti·ªÅn x·ª≠ l√Ω t·∫°o th√†nh vector s·ªë ƒë·ªÉ ƒë∆∞a v√†o t·ª´ ƒëi·ªÉn.\n","# √ù t∆∞·ªüng l√† : s·∫Ω t·∫°o th√†nh 1 t·∫≠p t·ª´ ƒëi·ªÉn, m·ªói t·ª´ s·∫Ω c√≥ 1 ch·ªâ s·ªë index trong t·ª´ ƒëi·ªÉn, ch√∫ng ta s·∫Ω thay t·ª´ ƒë√≥ b·∫±ng v·ªã tr√≠ c·ªßa n√≥ trong t·ª´ ƒëi·ªÉn\n","# B·ªüi v√¨ gi√° tr·ªã input c·ªßa m√¥ h√¨nh l√† c·ªë ƒë·ªãnh n√™n ch√∫ng ta s·∫Ω l·∫•y c√¢u d√†i nh·∫•t l√† ƒë·∫ßu v√†o m√¥ h√¨nh.\n","# Nh·ªØng c√¢u n√†o ng·∫Øn h∆°n th√¨ s·∫Ω t·ª± ƒë·ªông th√™m s·ªë 0 ƒë·ªÉ cho ƒë·ªß ƒë·ªô d√†i.\n","xLengths = [len(x.split(' ')) for x in totalX_train]\n","h = sorted(xLengths)  #sorted lengths\n","maxLength =h[len(h)-1]\n","print(\"max input length is: \",maxLength)\n","input_tokenizer = Tokenizer(filters=\"\",oov_token=\"UNK\")\n","input_tokenizer.fit_on_texts(totalX_train)\n","input_vocab_size = len(input_tokenizer.word_index) + 1\n","word_index = input_tokenizer.word_index\n","print(\"input_vocab_size:\",input_vocab_size)\n","totalX = np.array(pad_sequences(input_tokenizer.texts_to_sequences(totalX_train), maxlen=maxLength,padding=\"post\"))\n","textArray_dev = np.array(pad_sequences(input_tokenizer.texts_to_sequences(totalX_dev), maxlen=maxLength,padding=\"post\"))\n","print(totalX_train[0])\n","print(totalX[0])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Td-ML6bqWdBE"},"source":["# H√†m kh·ªüi t·∫°o gi√° tr·ªã embedding ƒë∆∞·ª£c l·∫•y t·ª´ pretrained word embedding.\n","# Nghƒ©a c√°c t·∫≠p t·ª´ v·ª±ng th√¨ t·ª´ n√†o c√≥ trong b·ªô pre-trained embedding s·∫Ω thay th·∫ø c√≤n kh√¥ng c√≥ th√¨ s·∫Ω kh·ªüi t·ªça l√† vector 0.\n","def generate_embedding(word_index, model_embedding,EMBEDDING_DIM):\n","    count6 = 0\n","    countNot6 = 0\n","    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM)) \n","    list_oov = []\n","    word_is_trained = []\n","    for word, i in word_index.items():\n","        try:\n","            embedding_vector = model_embedding[word]\n","            word_is_trained.append(word)\n","        except:\n","            list_oov.append(word)\n","            countNot6 +=1\n","            continue\n","        if embedding_vector is not None:\n","            count6 +=1\n","            embedding_matrix[i] = embedding_vector\n","    \n","    print('Number of words in pre-train embedding: ' + str(count6))\n","    print('Number of words not in pre-train embedding: ' + str(countNot6))\n","    print(list_oov)\n","    return embedding_matrix,word_is_trained"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HRIghLqMBuhV"},"source":["# Ch·∫°y h√†m kh·ªüi t·∫°o embedding v√† hi·ªán th·ªã c√°c t·ª´ c√≥ trong embedding.\n","embedding_matrix,word_is_trained = generate_embedding(word_index,word_embedding,EMBEDDING_DIM)\n","print(len(word_is_trained))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8b-4SQdfzFQ8"},"source":["# Kh·ªüi t·∫°o m√¥ h√¨nh CNN \n","import pickle\n","he_normal_initializer = keras.initializers.he_normal(seed=1234)\n","glorot_normal_initializer = keras.initializers.glorot_uniform(seed=1995)\n","recurrent_units = 128 # best 64\n","filter_nums = 128 # best 128\n","\n","\n","def build_model3(sequence_max_length,EMBEDDING_DIM):\n","        inputs  = Input(shape=(sequence_max_length, ), dtype='float64', name='inputs')    \n","        embedding_layer = Embedding(input_vocab_size,EMBEDDING_DIM,input_length=sequence_max_length, trainable=True, weights=[embedding_matrix],name = 'word_emb_DOMAIN')(inputs)\n","        \n","        conv_1 = Conv1D(filter_nums, 3, kernel_initializer=he_normal_initializer, padding=\"same\", activation=\"relu\")(embedding_layer)\n","        conv_2 = Conv1D(filter_nums, 4, kernel_initializer=he_normal_initializer, padding=\"same\", activation=\"relu\")(embedding_layer)\n","        conv_3 = Conv1D(filter_nums, 5, kernel_initializer=he_normal_initializer, padding=\"same\", activation=\"relu\")(embedding_layer)\n","        \n","        maxpool_1 = GlobalMaxPooling1D()(conv_1)\n","        maxpool_2 = GlobalMaxPooling1D()(conv_2)\n","        maxpool_3 = GlobalMaxPooling1D()(conv_3)\n","        v0_col = Concatenate(axis=1)([maxpool_1, maxpool_2, maxpool_3])     \n","         \n","        fc2 = Dropout(0.2)(Dense(300, name = 'dense_2',trainable=True)(v0_col))\n","         \n","        output1 = Dense(4,name=\"output1\", activation='softmax')(fc2)\n","        output2 = Dense(4,name=\"output2\", activation='softmax')(fc2)\n","        output3 = Dense(4,name=\"output3\", activation='softmax')(fc2)\n","        output4 = Dense(4,name=\"output4\", activation='softmax')(fc2)\n","        output5 = Dense(4,name=\"output5\", activation='softmax')(fc2)\n","        output6 = Dense(4,name=\"output6\", activation='softmax')(fc2)\n","        output7 = Dense(4,name=\"output7\", activation='softmax')(fc2)\n","        output8 = Dense(4,name=\"output8\", activation='softmax')(fc2)\n","        output9 = Dense(4,name=\"output9\", activation='softmax')(fc2)\n","        output10 = Dense(4,name=\"output10\", activation='softmax')(fc2)\n","        output11 = Dense(4,name=\"output11\", activation='softmax')(fc2)\n","        output12 = Dense(4,name=\"output12\", activation='softmax')(fc2)\n","        \n","\n","        model = Model(inputs=inputs, outputs=[output1,output2,output3,output4,output5,output6,output7,output8,output9,output10,output11,output12])\n","        \n","        tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n","        \n","        list_loss = ['categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy']\n","        model.compile(loss = list_loss, optimizer=\"adam\", metrics=['accuracy'])\n","\n","        list_total_Y_train  = [np.array(list_output_train[0]),np.array(list_output_train[1]),np.array(list_output_train[2]),np.array(list_output_train[3]),np.array(list_output_train[4]),np.array(list_output_train[5]),np.array(list_output_train[6]),np.array(list_output_train[7]),np.array(list_output_train[8]),np.array(list_output_train[9]),np.array(list_output_train[10]),np.array(list_output_train[11])]\n","        list_total_Y_dev = [np.array(list_output_dev[0]),np.array(list_output_dev[1]),np.array(list_output_dev[2]),np.array(list_output_dev[3]),np.array(list_output_dev[4]),np.array(list_output_dev[5]),np.array(list_output_dev[6]),np.array(list_output_dev[7]),np.array(list_output_dev[8]),np.array(list_output_dev[9]),np.array(list_output_dev[10]),np.array(list_output_dev[11])]       \n","        history = model.fit(totalX, list_total_Y_train, validation_data = [textArray_dev,list_total_Y_dev],batch_size=50, epochs=100,callbacks=[tensorBoardCallback])\n","        plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n","        return model\n","\n","model = build_model3(maxLength,EMBEDDING_DIM)\n","\n","model.save(\"ResCNN_model.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cQwJ75p_WdBJ"},"source":["# H√†m d·ª± ƒëo√°n tr√™n d·ªØ li·ªáu t·∫≠p test.\n","textArray_test = np.array(pad_sequences(input_tokenizer.texts_to_sequences(totalX_test), maxlen=maxLength,padding=\"post\"))\n","test_length = len(totalX_test)\n","\n","print(totalX_test[0])\n","print(textArray_test[0])\n","count = 0\n","textPrint = ''\n","textREsult = []\n","for iz,item in enumerate(textArray_test):\n","    predicted = model.predict([np.expand_dims(item, axis=0)])\n","    s = ''\n","    for i, predict in enumerate(predicted):\n","        index2, value = max(enumerate(predict[0]), key=operator.itemgetter(1))\n","        if index2 == 1:\n","            s+= '{' + str(categories[i]) + ', positive}, '\n","        elif index2 == 2:\n","            s+= '{' + str(categories[i]) + ', neutral}, '\n","        elif index2 == 3:\n","            s+= '{' + str(categories[i]) + ', negative}, '\n","    textPrint += '#' + str(count +1)+'\\n'\n","    textPrint += document_X_test[iz] + '\\n'\n","    textPrint += s[:len(s)-2] +'\\n\\n'\n","    count +=1\n","with open('CNN.txt','w',encoding = 'utf8') as file:\n","    file.write(textPrint)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ovXMr_dt1H-J"},"source":["# H√†m d·ª± ƒëo√°n tr√™n 1 d·ªØ li·ªáu ƒë·∫ßu v√†o\n","input_text = \"- M√πa ƒë√¥ng r·ªìi c√≤n g√¨ tuy·ªát v·ªùi h∆°n l√† ƒÉn ·ªëc v·ªõi ƒÉn ngao üòç - Nguy·ªÖn Khi·∫øt l√† ng√µ c·∫Øt ngang Ph√∫c T√¢n ·∫•y c√°c b·∫°n ·∫°. Ch·ªó m√† nhi·ªÅu nhi·ªÅu h√†ng karaoke. - ƒÇn ·ªü ƒë√¢y th√¨ h∆°i xa m·ªôt ch√∫t nh∆∞ng √¥ii th√¥i ngon th√¥i r·ªìi ƒë·∫£m b·∫£o kh√¥ng m·∫•t c√¥ng l√™n ƒë√¢y ƒÉn ƒë√¢u. - Gi√° ch·ªâ 30k-40k cho 1 b√°t ngao h·∫•p ƒë·∫ßy √∫ ·ª• lu√¥n üíì- Ch·ªó nh∆∞ ·∫£nh m√¨nh ƒÉn h·∫øt c√≥ 100k huhu m√† 2 ng ƒÉn no ch·∫øt ƒëi ƒëc. Coca c√≥ 5k 1 chai tr√† ƒë√° th√¨ free lu√¥n nh√© c√°c b·∫°n üòç - B√°c ch·ªß qu√°n hi·ªÅn l·∫Øm lu√¥n √Ω. Nhi·ªÅu l√∫c m√¨nh g·ªçi nhi·ªÅu ƒÉn ch·∫£ h·∫øt xong b√°c √Ω c√≤n ch·∫£ l·∫•y ti·ªÅn c∆° üò≠üò©\"\n","input_clean = clean_doc(input_text)\n","input_index = np.array(pad_sequences(input_tokenizer.texts_to_sequences([input_clean]), maxlen=maxLength,padding=\"post\"))\n","\n","predicted = model.predict([np.expand_dims(input_index[0], axis=0)])\n","output = ''\n","for i, predict in enumerate(predicted):\n","    index2, value = max(enumerate(predict[0]), key=operator.itemgetter(1))\n","    if index2 == 1:\n","        output+= '{' + str(categories[i]) + ', positive}, '\n","    elif index2 == 2:\n","        output+= '{' + str(categories[i]) + ', neutral}, '\n","    elif index2 == 3:\n","        output+= '{' + str(categories[i]) + ', negative}, '\n","output = output[:-2]\n","print(\"ƒê·∫ßu v√†o: \", input_text)\n","print(\"D·ª± ƒëo√°n: \", output)"],"execution_count":null,"outputs":[]}]}