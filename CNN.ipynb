{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"wgGywTOdWdAr"},"source":["# Cài đặt các thư viện cần thiết để chạy chương trình\n","%tensorflow_version 1.x\n","!pip install pyvi\n","!pip install ftfy\n","!pip install gensim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jqTDegKl3aDo"},"source":["# Đường dẫn được lưu trên google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","path_train ='/content/gdrive/My Drive/ABSA/Dataset/Train.txt'\n","path_dev ='/content/gdrive/My Drive/ABSA/Dataset/Dev.txt'\n","path_test ='/content/gdrive/My Drive/ABSA/Dataset/Test.txt'\n","\n","\n","# Các hàm embedding khác nhau theo thử nghiệm của bài báo\n","embedding_path = '/content/gdrive/My Drive/ABSA/Embedding/W2V_ner.vec'\n","#embedding_path = '/content/gdrive/My Drive/ABSA/Embedding/Bert_Base_ner.vec'\n","#embedding_path = '/content/gdrive/My Drive/ABSA/Embedding/ELMO_ner.vec'\n","#embedding_path = '/content/gdrive/My Drive/ABSA/Embedding/MULTI_W_F_B_E.vec'\n","#embedding_path = '/content/gdrive/My Drive/ABSA/Embedding/W2V_C2V_ner.vec'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AN8rgbmrdKJP"},"source":["# -*- coding: utf-8 -*-\n","import numpy as np\n","import tensorflow as tf\n","import os, pickle, re, keras, sklearn, string\n","from keras.callbacks import *\n","from keras.preprocessing.text import *\n","from keras.preprocessing.sequence import *\n","from keras.models import load_model\n","from pyvi import ViTokenizer, ViPosTagger\n","from keras.models import Model\n","from keras.layers import *\n","from keras import optimizers\n","from keras.utils.vis_utils import plot_model\n","import operator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Skg7lUZ9LYAn"},"source":["# Hàm đọc pretrained embedding \n","import io\n","def load_vectors(fname):\n","    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n","    count_vocab = 0\n","    n, d = map(int, fin.readline().split())\n","    data = {}\n","    for line in fin:\n","        tokens = line.rstrip().split(' ')\n","        coefs = np.asarray(tokens[1:], dtype='float64')\n","        data[tokens[0]] = coefs\n","        count_vocab += 1\n","    print(\"Tổng số các từ vựng là: \" + str(count_vocab))\n","    return data\n","\n","word_embedding = load_vectors(embedding_path)\n","EMBEDDING_DIM = word_embedding['yêu'].shape[0]\n","print(\"Chiều của embedding : \", EMBEDDING_DIM)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LZCbMBgRWdAy"},"source":["class Review():\n","    def __init__(self,stt,text,label):\n","        self.stt = stt\n","        self.labels = label\n","        self.orginalText = text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L1_TFsm9WdA0"},"source":["# Các hàm tiền xử lý dữ liệu\n","def normalText(sent):\n","    sent = str(sent).replace('_',' ').replace('/',' trên ')\n","    sent = re.sub('-{2,}','',sent)\n","    sent = re.sub('\\\\s+',' ', sent)\n","    patPrice = r'([0-9]+k?(\\s?-\\s?)[0-9]+\\s?(k|K))|([0-9]+(.|,)?[0-9]+\\s?(triệu|ngàn|trăm|k|K|))|([0-9]+(.[0-9]+)?Ä‘)|([0-9]+k)'\n","    patURL = r\"(?:http://|www.)[^\\\"]+\"\n","    sent = re.sub(patURL,'website',sent)\n","    sent = re.sub(patPrice, ' giá_tiền ', sent)\n","    sent = re.sub('\\.+','.',sent)\n","    sent = re.sub('(hagtag\\\\s+)+',' hagtag ',sent)\n","    sent = re.sub('\\\\s+',' ',sent)\n","    return sent\n","\n","def deleteIcon(text):\n","    text = text.lower()\n","    s = ''\n","    pattern = r\"[a-zA-ZaăâbcdđeêghiklmnoôơpqrstuưvxyàằầbcdđèềghìklmnòồờpqrstùừvxỳáắấbcdđéếghíklmnóốớpqrstúứvxýảẳẩbcdđẻểghỉklmnỏổởpqrstủửvxỷạặậbcdđẹệghịklmnọộợpqrstụựvxỵãẵẫbcdđẽễghĩklmnõỗỡpqrstũữvxỹAĂÂBCDĐEÊGHIKLMNOÔƠPQRSTUƯVXYÀẰẦBCDĐÈỀGHÌKLMNÒỒỜPQRSTÙỪVXỲÁẮẤBCDĐÉẾGHÍKLMNÓỐỚPQRSTÚỨVXÝẠẶẬBCDĐẸỆGHỊKLMNỌỘỢPQRSTỤỰVXỴẢẲẨBCDĐẺỂGHỈKLMNỎỔỞPQRSTỦỬVXỶÃẴẪBCDĐẼỄGHĨKLMNÕỖỠPQRSTŨỮVXỸ,._]\"\n","    \n","    for char in text:\n","        if char !=' ':\n","            if len(re.findall(pattern, char)) != 0:\n","                s+=char\n","            elif char == '_':\n","                s+=char\n","        else:\n","            s+=char\n","    s = re.sub('\\\\s+',' ',s)\n","    return s.strip()\n","\n","\n","def clean_doc(doc):\n","    for punc in string.punctuation:\n","        doc = doc.replace(punc,' '+ punc + ' ')\n","    doc = normalText(doc)\n","    doc = deleteIcon(doc)\n","    doc = ViTokenizer.tokenize(doc)\n","    # Lowercase\n","    doc = doc.lower()\n","    # Removing multiple whitespaces\n","    doc = re.sub(r\"\\?\", \" \\? \", doc)\n","    # Remove numbers\n","    doc = re.sub(r\"[0-9]+\", \" num \", doc)\n","    # Split in tokens\n","    # Remove punctuation\n","    for punc in string.punctuation:\n","      if punc not in \"_\":\n","          doc = doc.replace(punc,' ')\n","    doc = re.sub('\\\\s+',' ',doc)\n","    return doc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Usy-4ygPWdA2"},"source":["# hàm đọc dữ liệu lên từ file .txt theo định dạng của dữ liệu\n","from ftfy import fix_text\n","def read_files(filename):\n","    documents = list()\n","    count = 0\n","    with open(filename,'r',encoding='utf8') as file:\n","        text1 = file.read()\n","        for item in text1.split('\\n'):\n","            if item != '':\n","                item = item.strip()\n","                if count == 0:\n","                    stt = fix_text(item)\n","                    count += 1\n","                elif count == 1:\n","                    text = fix_text(item)\n","                    count += 1\n","                elif count == 2:\n","                    labels = fix_text(item)\n","                    count = 0\n","                    reviewtemp = Review(stt, text, labels)\n","                    documents.append(reviewtemp)\n","    return documents"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fyIQeZi-WdA4"},"source":["# Hàm chuyển các nhãn thành các vector đưa vào mô hình máy học\n","def to_category_vector(label):\n","    vector = np.zeros(4).astype(np.float64)\n","    if label.strip() != '':\n","        if 'positive' in label:\n","            vector[1] = 1.0\n","        elif 'neutral' in label:\n","            vector[2] = 1.0\n","        elif 'negative' in label:\n","            vector[3] = 1.0\n","        else:\n","            vector[0] = 1.0\n","    else:\n","        vector[0] = 1.0\n","    return vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XvCVg0BsWdA6"},"source":["# Đoạn code này sẽ chuyển tất cả nhãn thành một danh sách các vector biểu diễn tương ứng\n","import re\n","def create_output(labels,categories,list_output):\n","    for i,category in enumerate(categories):\n","        if category in labels:\n","            try:\n","                output = re.findall('('+category+',\\s(positive|negative|neutral))',labels)[0][0]\n","            except:\n","                output = ''\n","            list_output[i].append(to_category_vector(output))\n","        else:\n","            list_output[i].append(to_category_vector(''))\n","    return list_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9XN2TgIzWdA8"},"source":["# Đoạn code đọc dữ liệu từ các file .txt của tập dữ liệu train, dev, test. \n","# Sau đó chuyển nhãn dưới dạng text thành dạng vector lưu vào biến.\n","\n","listLabel = 'FOOD#STYLE&OPTIONS,FOOD#PRICES,FOOD#QUALITY,DRINKS#STYLE&OPTIONS,DRINKS#QUALITY,DRINKS#PRICES,RESTAURANT#GENERAL,RESTAURANT#MISCELLANEOUS,RESTAURANT#PRICES,LOCATION#GENERAL,SERVICE#GENERAL,AMBIENCE#GENERAL'\n","categories = listLabel.split(',')\n","print(len(categories))\n","document_X_train = []\n","document_Y_train = []\n","document_X_dev = []\n","document_Y_dev = []\n","document_X_test = []\n","document_Y_test = []\n","\n","output_train1 = []\n","output_train1 = []\n","output_train2 = []\n","output_train3 = []\n","output_train4 = []\n","output_train5 = []\n","output_train6 = []\n","output_train7 = []\n","output_train8 = []\n","output_train9 = []\n","output_train10 = []\n","output_train11 = []\n","output_train12 = []\n","\n","list_output_train = list()\n","list_output_train.append(output_train1)\n","list_output_train.append(output_train2)\n","list_output_train.append(output_train3)\n","list_output_train.append(output_train4)\n","list_output_train.append(output_train5)\n","list_output_train.append(output_train6)\n","list_output_train.append(output_train7)\n","list_output_train.append(output_train8)\n","list_output_train.append(output_train9)\n","list_output_train.append(output_train10)\n","list_output_train.append(output_train11)\n","list_output_train.append(output_train12)\n","\n","for review in read_files(path_train):\n","    try:\n","        document_X_train.append(review.orginalText)\n","        list_output_train = create_output(review.labels, categories,list_output_train)\n","    except:\n","        print(review.labels)\n","        print(review.orginalText)\n","\n","output_dev1 = []\n","output_dev2 = []\n","output_dev3 = []\n","output_dev4 = []\n","output_dev5 = []\n","output_dev6 = []\n","output_dev7 = []\n","output_dev8 = []\n","output_dev9 = []\n","output_dev10 = []\n","output_dev11 = []\n","output_dev12 = []\n","list_output_dev = list()\n","list_output_dev.append(output_dev1)\n","list_output_dev.append(output_dev2)\n","list_output_dev.append(output_dev3)\n","list_output_dev.append(output_dev4)\n","list_output_dev.append(output_dev5)\n","list_output_dev.append(output_dev6)\n","list_output_dev.append(output_dev7)\n","list_output_dev.append(output_dev8)\n","list_output_dev.append(output_dev9)\n","list_output_dev.append(output_dev10)\n","list_output_dev.append(output_dev11)\n","list_output_dev.append(output_dev12)\n","for review in read_files(path_dev):\n","    document_X_dev.append(review.orginalText)\n","    list_output_dev = create_output(review.labels, categories,list_output_dev)\n","\n","for review in read_files(path_test):\n","    document_X_test.append(review.orginalText)    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fqYyTPmLWdA_"},"source":["# Chạy hàm tiền xử lý dữ liệu (clean_doc) trên toàn bộ tập dữ liệu train và tập dữ liệu test.\n","totalX_train = []\n","totalY_train = np.array(document_Y_train)\n","\n","totalX_test = []\n","totalY_test = np.array(document_Y_test)\n","\n","totalX_dev = []\n","totalY_dev = np.array(document_Y_dev)\n","\n","for i, doc in enumerate(document_X_train):\n","    totalX_train.append(clean_doc(doc))\n","for i, doc in enumerate(document_X_test):\n","    totalX_test.append(clean_doc(doc))\n","\n","for i, doc in enumerate(document_X_dev):\n","    totalX_dev.append(clean_doc(doc))\n","print(totalX_train[0])\n","print(totalX_train[1])\n","print(totalX_train[2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0EB_NYPlWdBC"},"source":["# Đây là hàm chuyển các dữ liệu văn bản đã được tiền xử lý tạo thành vector số để đưa vào từ điển.\n","# Ý tưởng là : sẽ tạo thành 1 tập từ điển, mỗi từ sẽ có 1 chỉ số index trong từ điển, chúng ta sẽ thay từ đó bằng vị trí của nó trong từ điển\n","# Bởi vì giá trị input của mô hình là cố định nên chúng ta sẽ lấy câu dài nhất là đầu vào mô hình.\n","# Những câu nào ngắn hơn thì sẽ tự động thêm số 0 để cho đủ độ dài.\n","xLengths = [len(x.split(' ')) for x in totalX_train]\n","h = sorted(xLengths)  #sorted lengths\n","maxLength =h[len(h)-1]\n","print(\"max input length is: \",maxLength)\n","input_tokenizer = Tokenizer(filters=\"\",oov_token=\"UNK\")\n","input_tokenizer.fit_on_texts(totalX_train)\n","input_vocab_size = len(input_tokenizer.word_index) + 1\n","word_index = input_tokenizer.word_index\n","print(\"input_vocab_size:\",input_vocab_size)\n","totalX = np.array(pad_sequences(input_tokenizer.texts_to_sequences(totalX_train), maxlen=maxLength,padding=\"post\"))\n","textArray_dev = np.array(pad_sequences(input_tokenizer.texts_to_sequences(totalX_dev), maxlen=maxLength,padding=\"post\"))\n","print(totalX_train[0])\n","print(totalX[0])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Td-ML6bqWdBE"},"source":["# Hàm khởi tạo giá trị embedding được lấy từ pretrained word embedding.\n","# Nghĩa các tập từ vựng thì từ nào có trong bộ pre-trained embedding sẽ thay thế còn không có thì sẽ khởi tọa là vector 0.\n","def generate_embedding(word_index, model_embedding,EMBEDDING_DIM):\n","    count6 = 0\n","    countNot6 = 0\n","    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM)) \n","    list_oov = []\n","    word_is_trained = []\n","    for word, i in word_index.items():\n","        try:\n","            embedding_vector = model_embedding[word]\n","            word_is_trained.append(word)\n","        except:\n","            list_oov.append(word)\n","            countNot6 +=1\n","            continue\n","        if embedding_vector is not None:\n","            count6 +=1\n","            embedding_matrix[i] = embedding_vector\n","    \n","    print('Number of words in pre-train embedding: ' + str(count6))\n","    print('Number of words not in pre-train embedding: ' + str(countNot6))\n","    print(list_oov)\n","    return embedding_matrix,word_is_trained"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HRIghLqMBuhV"},"source":["# Chạy hàm khởi tạo embedding và hiện thị các từ có trong embedding.\n","embedding_matrix,word_is_trained = generate_embedding(word_index,word_embedding,EMBEDDING_DIM)\n","print(len(word_is_trained))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8b-4SQdfzFQ8"},"source":["# Khởi tạo mô hình CNN \n","import pickle\n","he_normal_initializer = keras.initializers.he_normal(seed=1234)\n","glorot_normal_initializer = keras.initializers.glorot_uniform(seed=1995)\n","recurrent_units = 128 # best 64\n","filter_nums = 128 # best 128\n","\n","\n","def build_model3(sequence_max_length,EMBEDDING_DIM):\n","        inputs  = Input(shape=(sequence_max_length, ), dtype='float64', name='inputs')    \n","        embedding_layer = Embedding(input_vocab_size,EMBEDDING_DIM,input_length=sequence_max_length, trainable=True, weights=[embedding_matrix],name = 'word_emb_DOMAIN')(inputs)\n","        \n","        conv_1 = Conv1D(filter_nums, 3, kernel_initializer=he_normal_initializer, padding=\"same\", activation=\"relu\")(embedding_layer)\n","        conv_2 = Conv1D(filter_nums, 4, kernel_initializer=he_normal_initializer, padding=\"same\", activation=\"relu\")(embedding_layer)\n","        conv_3 = Conv1D(filter_nums, 5, kernel_initializer=he_normal_initializer, padding=\"same\", activation=\"relu\")(embedding_layer)\n","        \n","        maxpool_1 = GlobalMaxPooling1D()(conv_1)\n","        maxpool_2 = GlobalMaxPooling1D()(conv_2)\n","        maxpool_3 = GlobalMaxPooling1D()(conv_3)\n","        v0_col = Concatenate(axis=1)([maxpool_1, maxpool_2, maxpool_3])     \n","         \n","        fc2 = Dropout(0.2)(Dense(300, name = 'dense_2',trainable=True)(v0_col))\n","         \n","        output1 = Dense(4,name=\"output1\", activation='softmax')(fc2)\n","        output2 = Dense(4,name=\"output2\", activation='softmax')(fc2)\n","        output3 = Dense(4,name=\"output3\", activation='softmax')(fc2)\n","        output4 = Dense(4,name=\"output4\", activation='softmax')(fc2)\n","        output5 = Dense(4,name=\"output5\", activation='softmax')(fc2)\n","        output6 = Dense(4,name=\"output6\", activation='softmax')(fc2)\n","        output7 = Dense(4,name=\"output7\", activation='softmax')(fc2)\n","        output8 = Dense(4,name=\"output8\", activation='softmax')(fc2)\n","        output9 = Dense(4,name=\"output9\", activation='softmax')(fc2)\n","        output10 = Dense(4,name=\"output10\", activation='softmax')(fc2)\n","        output11 = Dense(4,name=\"output11\", activation='softmax')(fc2)\n","        output12 = Dense(4,name=\"output12\", activation='softmax')(fc2)\n","        \n","\n","        model = Model(inputs=inputs, outputs=[output1,output2,output3,output4,output5,output6,output7,output8,output9,output10,output11,output12])\n","        \n","        tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n","        \n","        list_loss = ['categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy']\n","        model.compile(loss = list_loss, optimizer=\"adam\", metrics=['accuracy'])\n","\n","        list_total_Y_train  = [np.array(list_output_train[0]),np.array(list_output_train[1]),np.array(list_output_train[2]),np.array(list_output_train[3]),np.array(list_output_train[4]),np.array(list_output_train[5]),np.array(list_output_train[6]),np.array(list_output_train[7]),np.array(list_output_train[8]),np.array(list_output_train[9]),np.array(list_output_train[10]),np.array(list_output_train[11])]\n","        list_total_Y_dev = [np.array(list_output_dev[0]),np.array(list_output_dev[1]),np.array(list_output_dev[2]),np.array(list_output_dev[3]),np.array(list_output_dev[4]),np.array(list_output_dev[5]),np.array(list_output_dev[6]),np.array(list_output_dev[7]),np.array(list_output_dev[8]),np.array(list_output_dev[9]),np.array(list_output_dev[10]),np.array(list_output_dev[11])]       \n","        history = model.fit(totalX, list_total_Y_train, validation_data = [textArray_dev,list_total_Y_dev],batch_size=50, epochs=100,callbacks=[tensorBoardCallback])\n","        plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n","        return model\n","\n","model = build_model3(maxLength,EMBEDDING_DIM)\n","\n","model.save(\"ResCNN_model.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cQwJ75p_WdBJ"},"source":["# Hàm dự đoán trên dữ liệu tập test.\n","textArray_test = np.array(pad_sequences(input_tokenizer.texts_to_sequences(totalX_test), maxlen=maxLength,padding=\"post\"))\n","test_length = len(totalX_test)\n","\n","print(totalX_test[0])\n","print(textArray_test[0])\n","count = 0\n","textPrint = ''\n","textREsult = []\n","for iz,item in enumerate(textArray_test):\n","    predicted = model.predict([np.expand_dims(item, axis=0)])\n","    s = ''\n","    for i, predict in enumerate(predicted):\n","        index2, value = max(enumerate(predict[0]), key=operator.itemgetter(1))\n","        if index2 == 1:\n","            s+= '{' + str(categories[i]) + ', positive}, '\n","        elif index2 == 2:\n","            s+= '{' + str(categories[i]) + ', neutral}, '\n","        elif index2 == 3:\n","            s+= '{' + str(categories[i]) + ', negative}, '\n","    textPrint += '#' + str(count +1)+'\\n'\n","    textPrint += document_X_test[iz] + '\\n'\n","    textPrint += s[:len(s)-2] +'\\n\\n'\n","    count +=1\n","with open('CNN.txt','w',encoding = 'utf8') as file:\n","    file.write(textPrint)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ovXMr_dt1H-J"},"source":["# Hàm dự đoán trên 1 dữ liệu đầu vào\n","input_text = \"- Mùa đông rồi còn gì tuyệt vời hơn là ăn ốc với ăn ngao 😍 - Nguyễn Khiết là ngõ cắt ngang Phúc Tân ấy các bạn ạ. Chỗ mà nhiều nhiều hàng karaoke. - Ăn ở đây thì hơi xa một chút nhưng ôii thôi ngon thôi rồi đảm bảo không mất công lên đây ăn đâu. - Giá chỉ 30k-40k cho 1 bát ngao hấp đầy ú ụ luôn 💓- Chỗ như ảnh mình ăn hết có 100k huhu mà 2 ng ăn no chết đi đc. Coca có 5k 1 chai trà đá thì free luôn nhé các bạn 😍 - Bác chủ quán hiền lắm luôn ý. Nhiều lúc mình gọi nhiều ăn chả hết xong bác ý còn chả lấy tiền cơ 😭😩\"\n","input_clean = clean_doc(input_text)\n","input_index = np.array(pad_sequences(input_tokenizer.texts_to_sequences([input_clean]), maxlen=maxLength,padding=\"post\"))\n","\n","predicted = model.predict([np.expand_dims(input_index[0], axis=0)])\n","output = ''\n","for i, predict in enumerate(predicted):\n","    index2, value = max(enumerate(predict[0]), key=operator.itemgetter(1))\n","    if index2 == 1:\n","        output+= '{' + str(categories[i]) + ', positive}, '\n","    elif index2 == 2:\n","        output+= '{' + str(categories[i]) + ', neutral}, '\n","    elif index2 == 3:\n","        output+= '{' + str(categories[i]) + ', negative}, '\n","output = output[:-2]\n","print(\"Đầu vào: \", input_text)\n","print(\"Dự đoán: \", output)"],"execution_count":null,"outputs":[]}]}