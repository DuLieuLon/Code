{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wgGywTOdWdAr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: tensorflow in c:\\programdata\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.33.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.23.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (49.2.0.post20200714)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: pyvi in c:\\programdata\\anaconda3\\lib\\site-packages (0.1)\n",
      "Requirement already satisfied: sklearn-crfsuite in c:\\programdata\\anaconda3\\lib\\site-packages (from pyvi) (0.3.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from pyvi) (0.23.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
      "Requirement already satisfied: tabulate in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.8.7)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->pyvi) (4.47.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.9.7)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (1.18.5)\n",
      "Requirement already satisfied: ftfy in c:\\programdata\\anaconda3\\lib\\site-packages (5.8)\n",
      "Requirement already satisfied: wcwidth in c:\\programdata\\anaconda3\\lib\\site-packages (from ftfy) (0.2.5)\n",
      "Requirement already satisfied: gensim in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: Cython==0.29.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (0.29.14)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.18.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n"
     ]
    }
   ],
   "source": [
    "# Cài đặt các thư viện cần thiết để chạy chương trình\n",
    "#%tensorflow_version 1.x\n",
    "!pip install tensorflow\n",
    "!pip install pyvi\n",
    "!pip install ftfy\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jqTDegKl3aDo"
   },
   "outputs": [],
   "source": [
    "# Đường dẫn được lưu trên google drive\n",
    "# Dành cho goolge colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# path_train ='/content/gdrive/My Drive/ABSA/Dataset/Train.txt'\n",
    "# path_dev ='/content/gdrive/My Drive/ABSA/Dataset/Dev.txt'\n",
    "# path_test ='/content/gdrive/My Drive/ABSA/Dataset/Test.txt'\n",
    "\n",
    "\n",
    "# Các hàm embedding khác nhau theo thử nghiệm của bài báo\n",
    "# https://github.com/vietnlp/etnlp\n",
    "#embedding_path = '/content/gdrive/My Drive/ABSA/Embedding/W2V_ner.vec'\n",
    "#embedding_path = '/content/gdrive/My Drive/ABSA/Embedding/Bert_Base_ner.vec'\n",
    "#embedding_path = '/content/gdrive/My Drive/ABSA/Embedding/ELMO_ner.vec'\n",
    "#embedding_path = '/content/gdrive/My Drive/ABSA/Embedding/MULTI_W_F_B_E.vec'\n",
    "#embedding_path = '/content/gdrive/My Drive/ABSA/Embedding/W2V_C2V_ner.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I:\\\\Code'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dành cho jupyter notebook\n",
    "\n",
    "path_train = 'dataset/Train.txt'\n",
    "path_dev = 'dataset/Dev.txt'\n",
    "path_test = 'dataset/Test.txt'\n",
    "\n",
    "# Các hàm embedding khác nhau theo thử nghiệm của bài báo\n",
    "# https://github.com/vietnlp/etnlp\n",
    "#embedding_path = 'embedding/W2V_ner.vec'\n",
    "#embedding_path = 'embedding/Bert_Base_ner.vec'\n",
    "#embedding_path = 'embedding/ELMO_ner.vec'\n",
    "embedding_path = 'embedding/MULTI_W_F_B_E.vec'\n",
    "#embedding_path = 'embedding/W2V_C2V_ner.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AN8rgbmrdKJP"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os, pickle, re, keras, sklearn, string\n",
    "from keras.callbacks import *\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import *\n",
    "from keras.models import load_model\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Skg7lUZ9LYAn"
   },
   "outputs": [],
   "source": [
    "# Hàm đọc pretrained embedding \n",
    "import io\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    count_vocab = 0\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        coefs = np.asarray(tokens[1:], dtype='float64')\n",
    "        data[tokens[0]] = coefs\n",
    "        count_vocab += 1\n",
    "    print(\"Tổng số các từ vựng là: \" + str(count_vocab))\n",
    "    return data\n",
    "\n",
    "word_embedding = load_vectors(embedding_path)\n",
    "EMBEDDING_DIM = word_embedding['yêu'].shape[0]\n",
    "print(\"Chiều của embedding : \", EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZCbMBgRWdAy"
   },
   "outputs": [],
   "source": [
    "class Review():\n",
    "    def __init__(self,stt,text,label):\n",
    "        self.stt = stt\n",
    "        self.labels = label\n",
    "        self.orginalText = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1_TFsm9WdA0"
   },
   "outputs": [],
   "source": [
    "def normalText(sent):\n",
    "    sent = str(sent).replace('_',' ').replace('/',' trên ')\n",
    "    sent = re.sub('-{2,}','',sent)\n",
    "    sent = re.sub('\\\\s+',' ', sent)\n",
    "    patPrice = r'([0-9]+k?(\\s?-\\s?)[0-9]+\\s?(k|K))|([0-9]+(.|,)?[0-9]+\\s?(triệu|ngàn|trăm|k|K|))|([0-9]+(.[0-9]+)?Ä‘)|([0-9]+k)'\n",
    "    patURL = r\"(?:http://|www.)[^\\\"]+\"\n",
    "    sent = re.sub(patURL,'website',sent)\n",
    "    sent = re.sub(patPrice, ' giá_tiền ', sent)\n",
    "    sent = re.sub('\\.+','.',sent)\n",
    "    sent = re.sub('(hagtag\\\\s+)+',' hagtag ',sent)\n",
    "    sent = re.sub('\\\\s+',' ',sent)\n",
    "    return sent\n",
    "\n",
    "def deleteIcon(text):\n",
    "    text = text.lower()\n",
    "    s = ''\n",
    "    pattern = r\"[a-zA-ZaăâbcdđeêghiklmnoôơpqrstuưvxyàằầbcdđèềghìklmnòồờpqrstùừvxỳáắấbcdđéếghíklmnóốớpqrstúứvxýảẳẩbcdđẻểghỉklmnỏổởpqrstủửvxỷạặậbcdđẹệghịklmnọộợpqrstụựvxỵãẵẫbcdđẽễghĩklmnõỗỡpqrstũữvxỹAĂÂBCDĐEÊGHIKLMNOÔƠPQRSTUƯVXYÀẰẦBCDĐÈỀGHÌKLMNÒỒỜPQRSTÙỪVXỲÁẮẤBCDĐÉẾGHÍKLMNÓỐỚPQRSTÚỨVXÝẠẶẬBCDĐẸỆGHỊKLMNỌỘỢPQRSTỤỰVXỴẢẲẨBCDĐẺỂGHỈKLMNỎỔỞPQRSTỦỬVXỶÃẴẪBCDĐẼỄGHĨKLMNÕỖỠPQRSTŨỮVXỸ,._]\"\n",
    "    \n",
    "    for char in text:\n",
    "        if char !=' ':\n",
    "            if len(re.findall(pattern, char)) != 0:\n",
    "                s+=char\n",
    "            elif char == '_':\n",
    "                s+=char\n",
    "        else:\n",
    "            s+=char\n",
    "    s = re.sub('\\\\s+',' ',s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def clean_doc(doc):\n",
    "    for punc in string.punctuation:\n",
    "        doc = doc.replace(punc,' '+ punc + ' ')\n",
    "    doc = normalText(doc)\n",
    "    doc = deleteIcon(doc)\n",
    "    doc = ViTokenizer.tokenize(doc)\n",
    "    doc = doc.lower()\n",
    "    doc = re.sub(r\"\\?\", \" \\? \", doc)\n",
    "    doc = re.sub(r\"[0-9]+\", \" num \", doc)\n",
    "    for punc in string.punctuation:\n",
    "      if punc not in \"_\":\n",
    "          doc = doc.replace(punc,' ')\n",
    "    doc = re.sub('\\\\s+',' ',doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Usy-4ygPWdA2"
   },
   "outputs": [],
   "source": [
    "# hàm đọc dữ liệu lên từ file .txt theo định dạng của dữ liệu\n",
    "from ftfy import fix_text\n",
    "def read_files(filename):\n",
    "    documents = list()\n",
    "    count = 0\n",
    "    with open(filename,'r',encoding='utf8') as file:\n",
    "        text1 = file.read()\n",
    "        for item in text1.split('\\n'):\n",
    "            if item != '':\n",
    "                item = item.strip()\n",
    "                if count == 0:\n",
    "                    stt = fix_text(item)\n",
    "                    count += 1\n",
    "                elif count == 1:\n",
    "                    text = fix_text(item)\n",
    "                    count += 1\n",
    "                elif count == 2:\n",
    "                    labels = fix_text(item)\n",
    "                    count = 0\n",
    "                    reviewtemp = Review(stt, text, labels)\n",
    "                    documents.append(reviewtemp)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyIQeZi-WdA4"
   },
   "outputs": [],
   "source": [
    "# Hàm chuyển các nhãn thành các vector đưa vào mô hình máy học\n",
    "def to_category_vector(label):\n",
    "    vector = np.zeros(4).astype(np.float64)\n",
    "    if label.strip() != '':\n",
    "        if 'positive' in label:\n",
    "            vector[1] = 1.0\n",
    "        elif 'neutral' in label:\n",
    "            vector[2] = 1.0\n",
    "        elif 'negative' in label:\n",
    "            vector[3] = 1.0\n",
    "        else:\n",
    "            vector[0] = 1.0\n",
    "    else:\n",
    "        vector[0] = 1.0\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvCVg0BsWdA6"
   },
   "outputs": [],
   "source": [
    "# Đoạn code này sẽ chuyển tất cả nhãn thành một danh sách các vector biểu diễn tương ứng\n",
    "import re\n",
    "def create_output(labels,categories,list_output):\n",
    "    for i,category in enumerate(categories):\n",
    "        if category in labels:\n",
    "            try:\n",
    "                output = re.findall('('+category+',\\s(positive|negative|neutral))',labels)[0][0]\n",
    "            except:\n",
    "                output = ''\n",
    "            list_output[i].append(to_category_vector(output))\n",
    "        else:\n",
    "            list_output[i].append(to_category_vector(''))\n",
    "    return list_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XN2TgIzWdA8"
   },
   "outputs": [],
   "source": [
    "# Đoạn code đọc dữ liệu từ các file .txt của tập dữ liệu train, dev, test. \n",
    "# Sau đó chuyển nhãn dưới dạng text thành dạng vector lưu vào biến.\n",
    "\n",
    "listLabel = 'FOOD#STYLE&OPTIONS,FOOD#PRICES,FOOD#QUALITY,DRINKS#STYLE&OPTIONS,DRINKS#QUALITY,DRINKS#PRICES,RESTAURANT#GENERAL,RESTAURANT#MISCELLANEOUS,RESTAURANT#PRICES,LOCATION#GENERAL,SERVICE#GENERAL,AMBIENCE#GENERAL'\n",
    "categories = listLabel.split(',')\n",
    "print(len(categories))\n",
    "document_X_train = []\n",
    "document_Y_train = []\n",
    "document_X_dev = []\n",
    "document_Y_dev = []\n",
    "document_X_test = []\n",
    "document_Y_test = []\n",
    "\n",
    "output_train1 = []\n",
    "output_train1 = []\n",
    "output_train2 = []\n",
    "output_train3 = []\n",
    "output_train4 = []\n",
    "output_train5 = []\n",
    "output_train6 = []\n",
    "output_train7 = []\n",
    "output_train8 = []\n",
    "output_train9 = []\n",
    "output_train10 = []\n",
    "output_train11 = []\n",
    "output_train12 = []\n",
    "\n",
    "list_output_train = list()\n",
    "list_output_train.append(output_train1)\n",
    "list_output_train.append(output_train2)\n",
    "list_output_train.append(output_train3)\n",
    "list_output_train.append(output_train4)\n",
    "list_output_train.append(output_train5)\n",
    "list_output_train.append(output_train6)\n",
    "list_output_train.append(output_train7)\n",
    "list_output_train.append(output_train8)\n",
    "list_output_train.append(output_train9)\n",
    "list_output_train.append(output_train10)\n",
    "list_output_train.append(output_train11)\n",
    "list_output_train.append(output_train12)\n",
    "\n",
    "for review in read_files(path_train):\n",
    "    try:\n",
    "        document_X_train.append(review.orginalText)\n",
    "        list_output_train = create_output(review.labels, categories,list_output_train)\n",
    "    except:\n",
    "        print(review.labels)\n",
    "        print(review.orginalText)\n",
    "\n",
    "output_dev1 = []\n",
    "output_dev2 = []\n",
    "output_dev3 = []\n",
    "output_dev4 = []\n",
    "output_dev5 = []\n",
    "output_dev6 = []\n",
    "output_dev7 = []\n",
    "output_dev8 = []\n",
    "output_dev9 = []\n",
    "output_dev10 = []\n",
    "output_dev11 = []\n",
    "output_dev12 = []\n",
    "list_output_dev = list()\n",
    "list_output_dev.append(output_dev1)\n",
    "list_output_dev.append(output_dev2)\n",
    "list_output_dev.append(output_dev3)\n",
    "list_output_dev.append(output_dev4)\n",
    "list_output_dev.append(output_dev5)\n",
    "list_output_dev.append(output_dev6)\n",
    "list_output_dev.append(output_dev7)\n",
    "list_output_dev.append(output_dev8)\n",
    "list_output_dev.append(output_dev9)\n",
    "list_output_dev.append(output_dev10)\n",
    "list_output_dev.append(output_dev11)\n",
    "list_output_dev.append(output_dev12)\n",
    "for review in read_files(path_dev):\n",
    "    document_X_dev.append(review.orginalText)\n",
    "    list_output_dev = create_output(review.labels, categories,list_output_dev)\n",
    "\n",
    "for review in read_files(path_test):\n",
    "    document_X_test.append(review.orginalText)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqYyTPmLWdA_"
   },
   "outputs": [],
   "source": [
    "# Chạy hàm tiền xử lý dữ liệu (clean_doc) trên toàn bộ tập dữ liệu train và tập dữ liệu test.\n",
    "totalX_train = []\n",
    "totalY_train = np.array(document_Y_train)\n",
    "\n",
    "totalX_test = []\n",
    "totalY_test = np.array(document_Y_test)\n",
    "\n",
    "totalX_dev = []\n",
    "totalY_dev = np.array(document_Y_dev)\n",
    "\n",
    "for i, doc in enumerate(document_X_train):\n",
    "    totalX_train.append(clean_doc(doc))\n",
    "for i, doc in enumerate(document_X_test):\n",
    "    totalX_test.append(clean_doc(doc))\n",
    "\n",
    "for i, doc in enumerate(document_X_dev):\n",
    "    totalX_dev.append(clean_doc(doc))\n",
    "print(totalX_train[0])\n",
    "print(totalX_train[1])\n",
    "print(totalX_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EB_NYPlWdBC"
   },
   "outputs": [],
   "source": [
    "# Đây là hàm chuyển các dữ liệu văn bản đã được tiền xử lý tạo thành vector số để đưa vào từ điển.\n",
    "# Ý tưởng là : sẽ tạo thành 1 tập từ điển, mỗi từ sẽ có 1 chỉ số index trong từ điển, chúng ta sẽ thay từ đó bằng vị trí của nó trong từ điển\n",
    "# Bởi vì giá trị input của mô hình là cố định nên chúng ta sẽ lấy câu dài nhất là đầu vào mô hình.\n",
    "# Những câu nào ngắn hơn thì sẽ tự động thêm số 0 để cho đủ độ dài.\n",
    "xLengths = [len(x.split(' ')) for x in totalX_train]\n",
    "h = sorted(xLengths)  #sorted lengths\n",
    "maxLength =h[len(h)-1]\n",
    "print(\"max input length is: \",maxLength)\n",
    "input_tokenizer = Tokenizer(filters=\"\",oov_token=\"UNK\")\n",
    "input_tokenizer.fit_on_texts(totalX_train)\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "word_index = input_tokenizer.word_index\n",
    "print(\"input_vocab_size:\",input_vocab_size)\n",
    "totalX = np.array(pad_sequences(input_tokenizer.texts_to_sequences(totalX_train), maxlen=maxLength,padding=\"post\"))\n",
    "textArray_dev = np.array(pad_sequences(input_tokenizer.texts_to_sequences(totalX_dev), maxlen=maxLength,padding=\"post\"))\n",
    "print(totalX_train[0])\n",
    "print(totalX[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Td-ML6bqWdBE"
   },
   "outputs": [],
   "source": [
    "# Hàm khởi tạo giá trị embedding được lấy từ pretrained word embedding.\n",
    "# Nghĩa các tập từ vựng thì từ nào có trong bộ pre-trained embedding sẽ thay thế còn không có thì sẽ khởi tọa là vector 0.\n",
    "def generate_embedding(word_index, model_embedding,EMBEDDING_DIM):\n",
    "    count6 = 0\n",
    "    countNot6 = 0\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM)) \n",
    "    list_oov = []\n",
    "    word_is_trained = []\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model_embedding[word]\n",
    "            word_is_trained.append(word)\n",
    "        except:\n",
    "            list_oov.append(word)\n",
    "            countNot6 +=1\n",
    "            continue\n",
    "        if embedding_vector is not None:\n",
    "            count6 +=1\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix,word_is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRIghLqMBuhV"
   },
   "outputs": [],
   "source": [
    "# Chạy hàm khởi tạo embedding và hiện thị các từ có trong embedding.\n",
    "embedding_matrix,word_is_trained = generate_embedding(word_index,word_embedding,EMBEDDING_DIM)\n",
    "print(len(word_is_trained))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8b-4SQdfzFQ8"
   },
   "outputs": [],
   "source": [
    "# Khởi tạo mô hình BiLSTM-CNN theo bài báo.\n",
    "import pickle\n",
    "he_normal_initializer = keras.initializers.he_normal(seed=1234)\n",
    "glorot_normal_initializer = keras.initializers.glorot_uniform(seed=1995)\n",
    "recurrent_units = 128 # best 64\n",
    "filter_nums = 128 # best 128\n",
    "\n",
    "\n",
    "def build_model3(sequence_max_length,EMBEDDING_DIM):\n",
    "        inputs  = Input(shape=(sequence_max_length, ), dtype='float64', name='inputs')    \n",
    "        embedding_layer_domain = Embedding(input_vocab_size,EMBEDDING_DIM,input_length=sequence_max_length, trainable=True, weights=[embedding_matrix],name = 'word_emb_DOMAIN')(inputs)\n",
    "\n",
    "        rnn_1 = Bidirectional(LSTM(recurrent_units, return_sequences=True) ,name='BiLSTM',trainable=True)(embedding_layer_domain)       \n",
    "        \n",
    "        conv_1 = Conv1D(filter_nums, 3, kernel_initializer=he_normal_initializer, name = \"conv1\", padding=\"same\", activation=\"relu\")(rnn_1)\n",
    "        conv_2 = Conv1D(filter_nums, 4, kernel_initializer=he_normal_initializer, name = \"conv2\", padding=\"same\", activation=\"relu\")(rnn_1)\n",
    "        conv_3 = Conv1D(filter_nums, 5, kernel_initializer=he_normal_initializer, name = \"conv3\", padding=\"same\", activation=\"relu\")(rnn_1)\n",
    "        \n",
    "        maxpool_1 = GlobalMaxPooling1D()(conv_1)\n",
    "        avg_1 = GlobalAveragePooling1D()(conv_1)\n",
    "        maxpool_2 = GlobalMaxPooling1D()(conv_2)\n",
    "        avg_2 = GlobalAveragePooling1D()(conv_2)\n",
    "        maxpool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "        avg_3 = GlobalAveragePooling1D()(conv_3)\n",
    "\n",
    "        v0_col = Concatenate(axis=1)([maxpool_1, maxpool_2, maxpool_3])\n",
    "        v2_col = Concatenate(axis=1)([avg_1, avg_2, avg_3])\n",
    "\n",
    "        merged_tensor = Concatenate(axis=1)([v0_col,v2_col])        \n",
    "        fc2 = Dropout(0.2)(Dense(300, name = 'dense_2',trainable=True)(merged_tensor))\n",
    "        \n",
    "        output1 = Dense(4,name=\"output1\", activation='softmax')(fc2)\n",
    "        output2 = Dense(4,name=\"output2\", activation='softmax')(fc2)\n",
    "        output3 = Dense(4,name=\"output3\", activation='softmax')(fc2)\n",
    "        output4 = Dense(4,name=\"output4\", activation='softmax')(fc2)\n",
    "        output5 = Dense(4,name=\"output5\", activation='softmax')(fc2)\n",
    "        output6 = Dense(4,name=\"output6\", activation='softmax')(fc2)\n",
    "        output7 = Dense(4,name=\"output7\", activation='softmax')(fc2)\n",
    "        output8 = Dense(4,name=\"output8\", activation='softmax')(fc2)\n",
    "        output9 = Dense(4,name=\"output9\", activation='softmax')(fc2)\n",
    "        output10 = Dense(4,name=\"output10\", activation='softmax')(fc2)\n",
    "        output11 = Dense(4,name=\"output11\", activation='softmax')(fc2)\n",
    "        output12 = Dense(4,name=\"output12\", activation='softmax')(fc2)\n",
    "        \n",
    "\n",
    "    \n",
    "        # define optimizer\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=[output1,output2,output3,output4,output5,output6,output7,output8,output9,output10,output11,output12])\n",
    "        \n",
    "        tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "        \n",
    "        list_loss = ['categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy','categorical_crossentropy']\n",
    "        model.compile(loss = list_loss, optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "        list_total_Y_train  = [np.array(list_output_train[0]),np.array(list_output_train[1]),np.array(list_output_train[2]),np.array(list_output_train[3]),np.array(list_output_train[4]),np.array(list_output_train[5]),np.array(list_output_train[6]),np.array(list_output_train[7]),np.array(list_output_train[8]),np.array(list_output_train[9]),np.array(list_output_train[10]),np.array(list_output_train[11])]\n",
    "        \n",
    "        list_total_Y_dev = [np.array(list_output_dev[0]),np.array(list_output_dev[1]),np.array(list_output_dev[2]),np.array(list_output_dev[3]),np.array(list_output_dev[4]),np.array(list_output_dev[5]),np.array(list_output_dev[6]),np.array(list_output_dev[7]),np.array(list_output_dev[8]),np.array(list_output_dev[9]),np.array(list_output_dev[10]),np.array(list_output_dev[11])]\n",
    "       \n",
    "        history = model.fit(totalX, list_total_Y_train, validation_data = [textArray_dev,list_total_Y_dev], shuffle = True,batch_size=50, epochs=100,callbacks=[tensorBoardCallback])\n",
    "        plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
    "        return model\n",
    "\n",
    "model = build_model3(maxLength,EMBEDDING_DIM)\n",
    "\n",
    "model.save(\"Res_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPf1sueP0lPB"
   },
   "outputs": [],
   "source": [
    "# Lưu mô hình BiLSTM-CNN đã huấn luyện trên thư ABSA trong google drive\n",
    "# Dành cho Google colab\n",
    "\n",
    "# import pickle\n",
    "# path_save ='/content/gdrive/My Drive/ABSA/'\n",
    "# model.save(path_save + 'BiLSTM_CNN_model.h5')\n",
    "\n",
    "# with open('/content/gdrive/My Drive/ABSA/input_tokenizer.pkl', 'wb') as fp:\n",
    "#     pickle.dump(input_tokenizer, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dành cho Jupyter Notebook\n",
    "import pickle\n",
    "path_save = 'model/'\n",
    "model.save(path_save + 'BiLSTM_CNN_model.h5')\n",
    "with open('model/input_tokenizer.pkl', 'wb') as fp:\n",
    "    pickle.dump(input_tokenizer, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQwJ75p_WdBJ"
   },
   "outputs": [],
   "source": [
    "# Hàm dự đoán trên dữ liệu tập test.\n",
    "textArray_test = np.array(pad_sequences(input_tokenizer.texts_to_sequences(totalX_test), maxlen=maxLength,padding=\"post\"))\n",
    "test_length = len(totalX_test)\n",
    "\n",
    "print(totalX_test[0])\n",
    "print(textArray_test[0])\n",
    "count = 0\n",
    "textPrint = ''\n",
    "textREsult = []\n",
    "for iz,item in enumerate(textArray_test):\n",
    "    predicted = model.predict([np.expand_dims(item, axis=0)])\n",
    "    s = ''\n",
    "    for i, predict in enumerate(predicted):\n",
    "        index2, value = max(enumerate(predict[0]), key=operator.itemgetter(1))\n",
    "        if index2 == 1:\n",
    "            s+= '{' + str(categories[i]) + ', positive}, '\n",
    "        elif index2 == 2:\n",
    "            s+= '{' + str(categories[i]) + ', neutral}, '\n",
    "        elif index2 == 3:\n",
    "            s+= '{' + str(categories[i]) + ', negative}, '\n",
    "    textPrint += '#' + str(count +1)+'\\n'\n",
    "    textPrint += document_X_test[iz] + '\\n'\n",
    "    textPrint += s[:len(s)-2] +'\\n\\n'\n",
    "    count +=1\n",
    "with open('BiLSTM_CNN.txt','w',encoding = 'utf8') as file:\n",
    "    file.write(textPrint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aitmdJbs0yLY"
   },
   "outputs": [],
   "source": [
    "#Dành cho colab\n",
    "\n",
    "# from keras.models import load_model\n",
    "# path_save ='/content/gdrive/My Drive/ABSA/'\n",
    "# model = load_model(path_save + 'BiLSTM_CNN_model.h5')\n",
    "\n",
    "# with open('/content/gdrive/My Drive/ABSA/input_tokenizer.pkl', 'rb') as fp:\n",
    "#     input_tokenizer = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dành cho Notebook\n",
    "\n",
    "from keras.models import load_model\n",
    "path_save ='model/'\n",
    "model = load_model(path_save + 'BiLSTM_CNN_model.h5')\n",
    "\n",
    "with open('model/input_tokenizer.pkl', 'rb') as fp:\n",
    "    input_tokenizer = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3BA9RNu17Dq"
   },
   "outputs": [],
   "source": [
    "# Hàm dự đoán trên 1 dữ liệu đầu vào\n",
    "input_text = \"Món ăn ngon và nhân viên phục vụ rất tích cực\"\n",
    "input_clean = clean_doc(input_text)\n",
    "input_index = np.array(pad_sequences(input_tokenizer.texts_to_sequences([input_clean]), maxlen=maxLength,padding=\"post\"))\n",
    "\n",
    "predicted = model.predict([np.expand_dims(input_index[0], axis=0)])\n",
    "output = ''\n",
    "for i, predict in enumerate(predicted):\n",
    "    index2, value = max(enumerate(predict[0]), key=operator.itemgetter(1))\n",
    "    if index2 == 1:\n",
    "        output+= '{' + str(categories[i]) + ', positive}, '\n",
    "    elif index2 == 2:\n",
    "        output+= '{' + str(categories[i]) + ', neutral}, '\n",
    "    elif index2 == 3:\n",
    "        output+= '{' + str(categories[i]) + ', negative}, '\n",
    "print(\"Đầu vào: \", input_text)\n",
    "print(\"Dự đoán: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BiLSTM_CNN.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
